import asyncio
import logging
import os
import statistics
import time
import concurrent.futures
from typing import Dict, Optional
import yaml
import aiohttp
from tabulate import tabulate
from core.utils.llm import create_instance as create_llm_instance
from config.settings import load_config

# è®¾ç½®å…¨å±€æ—¥å¿—çº§åˆ«ä¸º WARNINGï¼ŒæŠ‘åˆ¶ INFO çº§åˆ«æ—¥å¿—
logging.basicConfig(level=logging.WARNING)

description = "å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½æµ‹è¯•"


class LLMPerformanceTester:
    def __init__(self):
        self.config = load_config()
        # ä½¿ç”¨æ›´ç¬¦åˆæ™ºèƒ½ä½“åœºæ™¯çš„æµ‹è¯•å†…å®¹ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = self._load_system_prompt()
        self.test_sentences = self.config.get("module_test", {}).get(
            "test_sentences",
            [
                "ä½ å¥½ï¼Œæˆ‘ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½ï¼Œèƒ½å®‰æ…°ä¸€ä¸‹æˆ‘å—ï¼Ÿ",
                "å¸®æˆ‘æŸ¥ä¸€ä¸‹æ˜å¤©çš„å¤©æ°”å¦‚ä½•ï¼Ÿ",
                "æˆ‘æƒ³å¬ä¸€ä¸ªæœ‰è¶£çš„æ•…äº‹ï¼Œä½ èƒ½ç»™æˆ‘è®²ä¸€ä¸ªå—ï¼Ÿ",
                "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿä»Šå¤©æ˜¯æ˜ŸæœŸå‡ ï¼Ÿ",
                "æˆ‘æƒ³è®¾ç½®ä¸€ä¸ªæ˜å¤©æ—©ä¸Š8ç‚¹çš„é—¹é’Ÿæé†’æˆ‘å¼€ä¼š",
            ],
        )
        self.results = {}

    def _load_system_prompt(self) -> str:
        """åŠ è½½ç³»ç»Ÿæç¤ºè¯"""
        try:
            prompt_file = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), "agent-base-prompt.txt"
            )
            with open(prompt_file, "r", encoding="utf-8") as f:
                content = f.read()
                # æ›¿æ¢æ¨¡æ¿å˜é‡ä¸ºæµ‹è¯•å€¼
                content = content.replace(
                    "{{base_prompt}}", "ä½ æ˜¯å°æ™ºï¼Œä¸€ä¸ªèªæ˜å¯çˆ±çš„AIåŠ©æ‰‹"
                )
                content = content.replace(
                    "{{emojiList}}", "ğŸ˜€,ğŸ˜ƒ,ğŸ˜„,ğŸ˜,ğŸ˜Š,ğŸ˜,ğŸ¤”,ğŸ˜®,ğŸ˜±,ğŸ˜¢,ğŸ˜­,ğŸ˜´,ğŸ˜µ,ğŸ¤—,ğŸ™„"
                )
                content = content.replace("{{current_time}}", "2024å¹´8æœˆ17æ—¥ 12:30:45")
                content = content.replace("{{today_date}}", "2024å¹´8æœˆ17æ—¥")
                content = content.replace("{{today_weekday}}", "æ˜ŸæœŸå…­")
                content = content.replace("{{lunar_date}}", "ç”²è¾°å¹´ä¸ƒæœˆåå››")
                content = content.replace("{{local_address}}", "åŒ—äº¬å¸‚")
                content = content.replace("{{weather_info}}", "ä»Šå¤©æ™´ï¼Œ25-32â„ƒ")
                return content
        except Exception as e:
            print(f"æ— æ³•åŠ è½½ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶: {e}")
            return "ä½ æ˜¯å°æ™ºï¼Œä¸€ä¸ªèªæ˜å¯çˆ±çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨æ¸©æš–å‹å–„çš„è¯­æ°”å›å¤ç”¨æˆ·ã€‚"

    def _collect_response_sync(self, llm, messages, llm_name, sentence_start):
        """åŒæ­¥æ”¶é›†å“åº”æ•°æ®çš„è¾…åŠ©æ–¹æ³•"""
        chunks = []
        first_token_received = False
        first_token_time = None

        try:
            response_generator = llm.response("perf_test", messages)
            chunk_count = 0
            for chunk in response_generator:
                chunk_count += 1
                # æ¯å¤„ç†ä¸€å®šæ•°é‡çš„chunkå°±æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦åº”è¯¥ä¸­æ–­
                if chunk_count % 10 == 0:
                    # é€šè¿‡æ£€æŸ¥å½“å‰çº¿ç¨‹æ˜¯å¦è¢«æ ‡è®°ä¸ºä¸­æ–­æ¥æå‰é€€å‡º
                    import threading

                    if (
                        threading.current_thread().ident
                        != threading.main_thread().ident
                    ):
                        # å¦‚æœä¸æ˜¯ä¸»çº¿ç¨‹ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                        pass

                # æ£€æŸ¥chunkæ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
                chunk_str = str(chunk)
                if (
                    "å¼‚å¸¸" in chunk_str
                    or "é”™è¯¯" in chunk_str
                    or "502" in chunk_str.lower()
                ):
                    error_msg = chunk_str.lower()
                    print(f"{llm_name} å“åº”åŒ…å«é”™è¯¯ä¿¡æ¯: {error_msg}")
                    # æŠ›å‡ºä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„å¼‚å¸¸
                    raise Exception(chunk_str)

                if not first_token_received and chunk.strip() != "":
                    first_token_time = time.time() - sentence_start
                    first_token_received = True
                    print(f"{llm_name} é¦–ä¸ª Token: {first_token_time:.3f}s")
                chunks.append(chunk)
        except Exception as e:
            # æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_msg = str(e).lower()
            print(f"{llm_name} å“åº”æ”¶é›†å¼‚å¸¸: {error_msg}")
            # å¯¹äº502é”™è¯¯æˆ–ç½‘ç»œé”™è¯¯ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†
            if (
                "502" in error_msg
                or "bad gateway" in error_msg
                or "error code: 502" in error_msg
                or "å¼‚å¸¸" in str(e)
                or "é”™è¯¯" in str(e)
            ):
                raise e
            # å¯¹äºå…¶ä»–é”™è¯¯ï¼Œå¯ä»¥è¿”å›éƒ¨åˆ†ç»“æœ
            return chunks, first_token_time

        return chunks, first_token_time

    async def _check_ollama_service(self, base_url: str, model_name: str) -> bool:
        """å¼‚æ­¥æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(f"{base_url}/api/version") as response:
                    if response.status != 200:
                        print(f"Ollama æœåŠ¡æœªå¯åŠ¨æˆ–æ— æ³•è®¿é—®: {base_url}")
                        return False
                async with session.get(f"{base_url}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get("models", [])
                        if not any(model["name"] == model_name for model in models):
                            print(
                                f"Ollama æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ï¼Œè¯·å…ˆä½¿ç”¨ `ollama pull {model_name}` ä¸‹è½½"
                            )
                            return False
                    else:
                        print("æ— æ³•è·å– Ollama æ¨¡å‹åˆ—è¡¨")
                        return False
                return True
            except Exception as e:
                print(f"æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡: {str(e)}")
                return False

    async def _test_single_sentence(
        self, llm_name: str, llm, sentence: str
    ) -> Optional[Dict]:
        """æµ‹è¯•å•ä¸ªå¥å­çš„æ€§èƒ½"""
        try:
            print(f"{llm_name} å¼€å§‹æµ‹è¯•: {sentence[:20]}...")
            sentence_start = time.time()
            first_token_received = False
            first_token_time = None

            # æ„å»ºåŒ…å«ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": sentence},
            ]

            # ä½¿ç”¨asyncio.wait_forè¿›è¡Œè¶…æ—¶æ§åˆ¶
            try:
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # åˆ›å»ºå“åº”æ”¶é›†ä»»åŠ¡
                    future = executor.submit(
                        self._collect_response_sync,
                        llm,
                        messages,
                        llm_name,
                        sentence_start,
                    )

                    # ä½¿ç”¨asyncio.wait_forå®ç°è¶…æ—¶æ§åˆ¶
                    try:
                        response_chunks, first_token_time = await asyncio.wait_for(
                            asyncio.wrap_future(future), timeout=10.0
                        )
                    except asyncio.TimeoutError:
                        print(f"{llm_name} æµ‹è¯•è¶…æ—¶ï¼ˆ10ç§’ï¼‰ï¼Œè·³è¿‡")
                        # å¼ºåˆ¶å–æ¶ˆfuture
                        future.cancel()
                        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿çº¿ç¨‹æ± ä»»åŠ¡èƒ½å¤Ÿå“åº”å–æ¶ˆ
                        try:
                            await asyncio.wait_for(
                                asyncio.wrap_future(future), timeout=1.0
                            )
                        except (
                            asyncio.TimeoutError,
                            concurrent.futures.CancelledError,
                            Exception,
                        ):
                            # å¿½ç•¥æ‰€æœ‰å¼‚å¸¸ï¼Œç¡®ä¿ç¨‹åºç»§ç»­æ‰§è¡Œ
                            pass
                        return None

            except Exception as timeout_error:
                print(f"{llm_name} å¤„ç†å¼‚å¸¸: {timeout_error}")
                return None

            response_time = time.time() - sentence_start
            print(f"{llm_name} å®Œæˆå“åº”: {response_time:.3f}s")

            return {
                "name": llm_name,
                "type": "llm",
                "first_token_time": first_token_time,
                "response_time": response_time,
            }
        except Exception as e:
            error_msg = str(e).lower()
            # æ£€æŸ¥æ˜¯å¦ä¸º502é”™è¯¯æˆ–ç½‘ç»œé”™è¯¯
            if (
                "502" in error_msg
                or "bad gateway" in error_msg
                or "error code: 502" in error_msg
            ):
                print(f"{llm_name} é‡åˆ°502é”™è¯¯ï¼Œè·³è¿‡æµ‹è¯•")
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "502ç½‘ç»œé”™è¯¯",
                }
            print(f"{llm_name} å¥å­æµ‹è¯•å¤±è´¥: {str(e)}")
            return None

    async def _test_llm(self, llm_name: str, config: Dict) -> Dict:
        """å¼‚æ­¥æµ‹è¯•å•ä¸ª LLM æ€§èƒ½"""
        try:
            # å¯¹äº Ollamaï¼Œè·³è¿‡ api_key æ£€æŸ¥å¹¶è¿›è¡Œç‰¹æ®Šå¤„ç†
            if llm_name == "Ollama":
                base_url = config.get("base_url", "http://localhost:11434")
                model_name = config.get("model_name")
                if not model_name:
                    print("Ollama æœªé…ç½® model_name")
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "ç½‘ç»œé”™è¯¯",
                    }

                if not await self._check_ollama_service(base_url, model_name):
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "ç½‘ç»œé”™è¯¯",
                    }
            else:
                if "api_key" in config and any(
                    x in config["api_key"] for x in ["ä½ çš„", "placeholder", "sk-xxx"]
                ):
                    print(f"è·³è¿‡æœªé…ç½®çš„ LLM: {llm_name}")
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "é…ç½®é”™è¯¯",
                    }

            # è·å–å®é™…ç±»å‹ï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
            module_type = config.get("type", llm_name)
            llm = create_llm_instance(module_type, config)

            # ç»Ÿä¸€ä½¿ç”¨ UTF-8 ç¼–ç 
            test_sentences = [
                s.encode("utf-8").decode("utf-8") for s in self.test_sentences
            ]

            # åˆ›å»ºæ‰€æœ‰å¥å­çš„æµ‹è¯•ä»»åŠ¡
            sentence_tasks = []
            for sentence in test_sentences:
                sentence_tasks.append(
                    self._test_single_sentence(llm_name, llm, sentence)
                )

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å¥å­æµ‹è¯•ï¼Œå¹¶å¤„ç†å¯èƒ½çš„å¼‚å¸¸
            sentence_results = await asyncio.gather(
                *sentence_tasks, return_exceptions=True
            )

            # å¤„ç†ç»“æœï¼Œè¿‡æ»¤æ‰å¼‚å¸¸å’ŒNoneå€¼
            valid_results = []
            for result in sentence_results:
                if isinstance(result, dict) and result is not None:
                    valid_results.append(result)
                elif isinstance(result, Exception):
                    error_msg = str(result).lower()
                    if "502" in error_msg or "bad gateway" in error_msg:
                        print(f"{llm_name} é‡åˆ°502é”™è¯¯ï¼Œè·³è¿‡è¯¥å¥å­æµ‹è¯•")
                        return {
                            "name": llm_name,
                            "type": "llm",
                            "errors": 1,
                            "error_type": "502ç½‘ç»œé”™è¯¯",
                        }
                    else:
                        print(f"{llm_name} å¥å­æµ‹è¯•å¼‚å¸¸: {result}")

            if not valid_results:
                print(f"{llm_name} æ— æœ‰æ•ˆæ•°æ®ï¼Œå¯èƒ½é‡åˆ°ç½‘ç»œé—®é¢˜æˆ–é…ç½®é”™è¯¯")
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "ç½‘ç»œé”™è¯¯",
                }

            # æ£€æŸ¥æœ‰æ•ˆç»“æœæ•°é‡ï¼Œå¦‚æœå¤ªå°‘åˆ™è®¤ä¸ºæµ‹è¯•å¤±è´¥
            if len(valid_results) < len(test_sentences) * 0.3:  # è‡³å°‘è¦æœ‰30%çš„æˆåŠŸç‡
                print(
                    f"{llm_name} æˆåŠŸæµ‹è¯•å¥å­è¿‡å°‘({len(valid_results)}/{len(test_sentences)})ï¼Œå¯èƒ½ç½‘ç»œä¸ç¨³å®šæˆ–æ¥å£æœ‰é—®é¢˜"
                )
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "ç½‘ç»œé”™è¯¯",
                }

            first_token_times = [
                r["first_token_time"]
                for r in valid_results
                if r.get("first_token_time")
            ]
            response_times = [r["response_time"] for r in valid_results]

            # è¿‡æ»¤å¼‚å¸¸æ•°æ®ï¼ˆè¶…å‡º3ä¸ªæ ‡å‡†å·®çš„æ•°æ®ï¼‰
            if len(response_times) > 1:
                mean = statistics.mean(response_times)
                stdev = statistics.stdev(response_times)
                filtered_times = [t for t in response_times if t <= mean + 3 * stdev]
            else:
                filtered_times = response_times

            return {
                "name": llm_name,
                "type": "llm",
                "avg_response": sum(response_times) / len(response_times),
                "avg_first_token": (
                    sum(first_token_times) / len(first_token_times)
                    if first_token_times
                    else 0
                ),
                "success_rate": f"{len(valid_results)}/{len(test_sentences)}",
                "errors": 0,
            }
        except Exception as e:
            error_msg = str(e).lower()
            if "502" in error_msg or "bad gateway" in error_msg:
                print(f"LLM {llm_name} é‡åˆ°502é”™è¯¯ï¼Œè·³è¿‡æµ‹è¯•")
            else:
                print(f"LLM {llm_name} æµ‹è¯•å¤±è´¥: {str(e)}")
            error_type = "ç½‘ç»œé”™è¯¯"
            if "timeout" in str(e).lower():
                error_type = "è¶…æ—¶è¿æ¥"
            return {
                "name": llm_name,
                "type": "llm",
                "errors": 1,
                "error_type": error_type,
            }

    def _print_results(self):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\n" + "=" * 50)
        print("LLM æ€§èƒ½æµ‹è¯•ç»“æœ")
        print("=" * 50)

        if not self.results:
            print("æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
            return

        headers = ["æ¨¡å‹åç§°", "å¹³å‡å“åº”æ—¶é—´(s)", "é¦–Tokenæ—¶é—´(s)", "æˆåŠŸç‡", "çŠ¶æ€"]
        table_data = []

        # æ”¶é›†æ‰€æœ‰æ•°æ®å¹¶åˆ†ç±»
        valid_results = []
        error_results = []

        for name, data in self.results.items():
            if data["errors"] == 0:
                # æ­£å¸¸ç»“æœ
                avg_response = f"{data['avg_response']:.3f}"
                avg_first_token = (
                    f"{data['avg_first_token']:.3f}"
                    if data["avg_first_token"] > 0
                    else "-"
                )
                success_rate = data.get("success_rate", "N/A")
                status = "âœ… æ­£å¸¸"

                # ä¿å­˜ç”¨äºæ’åºçš„å€¼
                first_token_value = (
                    data["avg_first_token"]
                    if data["avg_first_token"] > 0
                    else float("inf")
                )

                valid_results.append(
                    {
                        "name": name,
                        "avg_response": avg_response,
                        "avg_first_token": avg_first_token,
                        "success_rate": success_rate,
                        "status": status,
                        "sort_key": first_token_value,
                    }
                )
            else:
                # é”™è¯¯ç»“æœ
                avg_response = "-"
                avg_first_token = "-"
                success_rate = "0/5"

                # è·å–å…·ä½“é”™è¯¯ç±»å‹
                error_type = data.get("error_type", "ç½‘ç»œé”™è¯¯")
                status = f"âŒ {error_type}"

                error_results.append(
                    [name, avg_response, avg_first_token, success_rate, status]
                )

        # æŒ‰é¦–Tokenæ—¶é—´å‡åºæ’åº
        valid_results.sort(key=lambda x: x["sort_key"])

        # å°†æ’åºåçš„æœ‰æ•ˆç»“æœè½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®
        for result in valid_results:
            table_data.append(
                [
                    result["name"],
                    result["avg_response"],
                    result["avg_first_token"],
                    result["success_rate"],
                    result["status"],
                ]
            )

        # å°†é”™è¯¯ç»“æœæ·»åŠ åˆ°è¡¨æ ¼æ•°æ®æœ«å°¾
        table_data.extend(error_results)

        print(tabulate(table_data, headers=headers, tablefmt="grid"))
        print("\næµ‹è¯•è¯´æ˜:")
        print("- æµ‹è¯•å†…å®¹ï¼šåŒ…å«å®Œæ•´ç³»ç»Ÿæç¤ºè¯çš„æ™ºèƒ½ä½“å¯¹è¯åœºæ™¯")
        print("- è¶…æ—¶æ§åˆ¶ï¼šå•ä¸ªè¯·æ±‚æœ€å¤§ç­‰å¾…æ—¶é—´ä¸º10ç§’")
        print("- é”™è¯¯å¤„ç†ï¼šè‡ªåŠ¨è·³è¿‡502é”™è¯¯å’Œç½‘ç»œå¼‚å¸¸çš„æ¨¡å‹")
        print("- æˆåŠŸç‡ï¼šæˆåŠŸå“åº”çš„å¥å­æ•°é‡/æ€»æµ‹è¯•å¥å­æ•°é‡")
        print("\næµ‹è¯•å®Œæˆï¼")

    async def run(self):
        """æ‰§è¡Œå…¨é‡å¼‚æ­¥æµ‹è¯•"""
        print("å¼€å§‹ç­›é€‰å¯ç”¨ LLM æ¨¡å—...")

        # åˆ›å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
        all_tasks = []

        # LLM æµ‹è¯•ä»»åŠ¡
        if self.config.get("LLM") is not None:
            for llm_name, config in self.config.get("LLM", {}).items():
                # æ£€æŸ¥é…ç½®æœ‰æ•ˆæ€§
                if llm_name == "CozeLLM":
                    if any(x in config.get("bot_id", "") for x in ["ä½ çš„"]) or any(
                        x in config.get("user_id", "") for x in ["ä½ çš„"]
                    ):
                        print(f"LLM {llm_name} æœªé…ç½® bot_id/user_idï¼Œå·²è·³è¿‡")
                        continue
                elif "api_key" in config and any(
                    x in config["api_key"] for x in ["ä½ çš„", "placeholder", "sk-xxx"]
                ):
                    print(f"LLM {llm_name} æœªé…ç½® api_keyï¼Œå·²è·³è¿‡")
                    continue

                # å¯¹äº Ollamaï¼Œå…ˆæ£€æŸ¥æœåŠ¡çŠ¶æ€
                if llm_name == "Ollama":
                    base_url = config.get("base_url", "http://localhost:11434")
                    model_name = config.get("model_name")
                    if not model_name:
                        print("Ollama æœªé…ç½® model_name")
                        continue

                    if not await self._check_ollama_service(base_url, model_name):
                        continue

                print(f"æ·»åŠ  LLM æµ‹è¯•ä»»åŠ¡: {llm_name}")
                all_tasks.append(self._test_llm(llm_name, config))

        print(f"\næ‰¾åˆ° {len(all_tasks)} ä¸ªå¯ç”¨ LLM æ¨¡å—")
        print("\nå¼€å§‹å¹¶å‘æµ‹è¯•æ‰€æœ‰æ¨¡å—...\n")

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æµ‹è¯•ä»»åŠ¡ï¼Œä½†ä¸ºæ¯ä¸ªä»»åŠ¡è®¾ç½®ç‹¬ç«‹è¶…æ—¶
        async def test_with_timeout(task, timeout=30):
            """ä¸ºæ¯ä¸ªæµ‹è¯•ä»»åŠ¡æ·»åŠ è¶…æ—¶ä¿æŠ¤"""
            try:
                return await asyncio.wait_for(task, timeout=timeout)
            except asyncio.TimeoutError:
                print(f"æµ‹è¯•ä»»åŠ¡è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œè·³è¿‡")
                return {
                    "name": "Unknown",
                    "type": "llm",
                    "errors": 1,
                    "error_type": "è¶…æ—¶è¿æ¥",
                }
            except Exception as e:
                print(f"æµ‹è¯•ä»»åŠ¡å¼‚å¸¸: {str(e)}")
                return {
                    "name": "Unknown",
                    "type": "llm",
                    "errors": 1,
                    "error_type": "ç½‘ç»œé”™è¯¯",
                }

        # ä¸ºæ¯ä¸ªä»»åŠ¡åŒ…è£…è¶…æ—¶ä¿æŠ¤
        protected_tasks = [test_with_timeout(task) for task in all_tasks]

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
        all_results = await asyncio.gather(*protected_tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        for result in all_results:
            if isinstance(result, dict):
                if result.get("errors") == 0:
                    self.results[result["name"]] = result
                else:
                    # å³ä½¿æœ‰é”™è¯¯ä¹Ÿè®°å½•ï¼Œç”¨äºæ˜¾ç¤ºå¤±è´¥çŠ¶æ€
                    if result.get("name") != "Unknown":
                        self.results[result["name"]] = result
            elif isinstance(result, Exception):
                print(f"æµ‹è¯•ç»“æœå¤„ç†å¼‚å¸¸: {str(result)}")

        # æ‰“å°ç»“æœ
        print("\nç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        self._print_results()


async def main():
    tester = LLMPerformanceTester()
    await tester.run()


if __name__ == "__main__":
    asyncio.run(main())
